{"version":3,"sources":["../src/llm/types.ts","../src/llm/client.ts"],"sourcesContent":["/**\n * LLM Types for gICM Agent Core\n * Enhanced with Opus 4.5 features: effort parameter, extended thinking, tool search\n */\n\nimport { z } from \"zod\";\n\n// =============================================================================\n// PROVIDER & MODEL\n// =============================================================================\n\nexport const LLMProviderSchema = z.enum([\"openai\", \"anthropic\", \"gemini\", \"deepseek\"]);\nexport type LLMProvider = z.infer<typeof LLMProviderSchema>;\n\n// =============================================================================\n// EFFORT LEVELS (Opus 4.5 exclusive)\n// =============================================================================\n\nexport const EffortLevelSchema = z.enum([\"low\", \"medium\", \"high\"]);\nexport type EffortLevel = z.infer<typeof EffortLevelSchema>;\n\n// =============================================================================\n// EXTENDED THINKING CONFIG\n// =============================================================================\n\nexport const ExtendedThinkingSchema = z.object({\n  enabled: z.boolean().default(false),\n  budgetTokens: z.number().min(1000).max(128000).default(8000),\n});\nexport type ExtendedThinking = z.infer<typeof ExtendedThinkingSchema>;\n\n// =============================================================================\n// MAIN LLM CONFIG\n// =============================================================================\n\nexport const LLMConfigSchema = z.object({\n  provider: LLMProviderSchema,\n  model: z.string().optional(),\n  apiKey: z.string(),\n  temperature: z.number().min(0).max(2).default(0.7),\n  maxTokens: z.number().default(4096),\n\n  // Opus 4.5 features\n  effort: EffortLevelSchema.optional(),\n  extendedThinking: ExtendedThinkingSchema.optional(),\n\n  // Prompt caching (90% savings)\n  promptCaching: z.boolean().optional(),\n});\nexport type LLMConfig = z.infer<typeof LLMConfigSchema>;\n\n// =============================================================================\n// MESSAGES\n// =============================================================================\n\nexport interface LLMMessage {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n}\n\n// =============================================================================\n// THINKING BLOCK (from extended thinking)\n// =============================================================================\n\nexport interface ThinkingBlock {\n  type: \"thinking\";\n  thinking: string;\n}\n\n// =============================================================================\n// RESPONSE\n// =============================================================================\n\nexport interface LLMResponse {\n  content: string;\n  thinking?: string; // Extended thinking output\n  usage?: {\n    promptTokens: number;\n    completionTokens: number;\n    thinkingTokens?: number;\n    cachedTokens?: number;\n    totalTokens: number;\n  };\n  finishReason?: string;\n  model?: string;\n  latencyMs?: number;\n}\n\n// =============================================================================\n// CLIENT INTERFACE\n// =============================================================================\n\nexport interface LLMClient {\n  chat(messages: LLMMessage[]): Promise<LLMResponse>;\n  complete(prompt: string): Promise<string>;\n  getConfig(): LLMConfig;\n}\n","/**\n * Universal LLM Client for gICM\n * Enhanced with Opus 4.5 features: effort parameter, extended thinking, prompt caching\n */\n\nimport type { LLMClient, LLMConfig, LLMMessage, LLMResponse, EffortLevel, LLMProvider } from \"./types.js\";\n\n// =============================================================================\n// RESPONSE INTERFACES\n// =============================================================================\n\ninterface OpenAIResponse {\n  choices: Array<{ message: { content: string }; finish_reason?: string }>;\n  usage?: { prompt_tokens: number; completion_tokens: number; total_tokens: number };\n  model?: string;\n}\n\ninterface AnthropicContentBlock {\n  type: \"text\" | \"thinking\";\n  text?: string;\n  thinking?: string;\n}\n\ninterface AnthropicResponse {\n  content: AnthropicContentBlock[];\n  usage?: {\n    input_tokens: number;\n    output_tokens: number;\n    cache_creation_input_tokens?: number;\n    cache_read_input_tokens?: number;\n  };\n  stop_reason?: string;\n  model?: string;\n}\n\ninterface GeminiResponse {\n  candidates?: Array<{ content?: { parts?: Array<{ text?: string }> } }>;\n  usageMetadata?: { promptTokenCount?: number; candidatesTokenCount?: number; totalTokenCount?: number };\n}\n\ninterface DeepSeekResponse {\n  choices: Array<{ message: { content: string }; finish_reason?: string }>;\n  usage?: { prompt_tokens: number; completion_tokens: number; total_tokens: number };\n  model?: string;\n}\n\n// =============================================================================\n// DEFAULT MODELS\n// =============================================================================\n\nconst DEFAULT_MODELS: Record<string, string> = {\n  openai: \"gpt-4o\",\n  anthropic: \"claude-opus-4-5-20251101\",\n  gemini: \"gemini-2.0-flash\",\n  deepseek: \"deepseek-chat\",\n};\n\n// Effort parameter max tokens adjustment\nconst EFFORT_MAX_TOKENS: Record<EffortLevel, number> = {\n  low: 2048,\n  medium: 8192,\n  high: 32000,\n};\n\n// =============================================================================\n// UNIVERSAL LLM CLIENT\n// =============================================================================\n\nexport class UniversalLLMClient implements LLMClient {\n  private config: LLMConfig;\n  private model: string;\n\n  constructor(config: LLMConfig) {\n    this.config = config;\n    this.model = config.model ?? DEFAULT_MODELS[config.provider];\n  }\n\n  getConfig(): LLMConfig {\n    return { ...this.config };\n  }\n\n  async chat(messages: LLMMessage[]): Promise<LLMResponse> {\n    const startTime = Date.now();\n\n    let response: LLMResponse;\n    switch (this.config.provider) {\n      case \"openai\":\n        response = await this.chatOpenAI(messages);\n        break;\n      case \"anthropic\":\n        response = await this.chatAnthropic(messages);\n        break;\n      case \"gemini\":\n        response = await this.chatGemini(messages);\n        break;\n      case \"deepseek\":\n        response = await this.chatDeepSeek(messages);\n        break;\n      default:\n        throw new Error(`Unsupported provider: ${this.config.provider}`);\n    }\n\n    response.latencyMs = Date.now() - startTime;\n    return response;\n  }\n\n  async complete(prompt: string): Promise<string> {\n    const response = await this.chat([{ role: \"user\", content: prompt }]);\n    return response.content;\n  }\n\n  // ===========================================================================\n  // OPENAI\n  // ===========================================================================\n\n  private async chatOpenAI(messages: LLMMessage[]): Promise<LLMResponse> {\n    const maxTokens = this.config.effort\n      ? EFFORT_MAX_TOKENS[this.config.effort]\n      : this.config.maxTokens;\n\n    const response = await fetch(\"https://api.openai.com/v1/chat/completions\", {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        Authorization: `Bearer ${this.config.apiKey}`,\n      },\n      body: JSON.stringify({\n        model: this.model,\n        messages,\n        temperature: this.config.temperature,\n        max_tokens: maxTokens,\n      }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`OpenAI API error: ${response.status} - ${error}`);\n    }\n\n    const data = (await response.json()) as OpenAIResponse;\n    return {\n      content: data.choices[0].message.content,\n      usage: {\n        promptTokens: data.usage?.prompt_tokens ?? 0,\n        completionTokens: data.usage?.completion_tokens ?? 0,\n        totalTokens: data.usage?.total_tokens ?? 0,\n      },\n      finishReason: data.choices[0].finish_reason,\n      model: data.model,\n    };\n  }\n\n  // ===========================================================================\n  // ANTHROPIC (with Opus 4.5 features)\n  // ===========================================================================\n\n  private async chatAnthropic(messages: LLMMessage[]): Promise<LLMResponse> {\n    const systemMessage = messages.find((m) => m.role === \"system\");\n    const userMessages = messages.filter((m) => m.role !== \"system\");\n\n    // Build request body\n    const body: Record<string, unknown> = {\n      model: this.model,\n      max_tokens: this.config.maxTokens,\n      messages: userMessages.map((m) => ({\n        role: m.role === \"assistant\" ? \"assistant\" : \"user\",\n        content: m.content,\n      })),\n    };\n\n    // System prompt with optional caching\n    if (systemMessage) {\n      if (this.config.promptCaching) {\n        body.system = [\n          {\n            type: \"text\",\n            text: systemMessage.content,\n            cache_control: { type: \"ephemeral\" },\n          },\n        ];\n      } else {\n        body.system = systemMessage.content;\n      }\n    }\n\n    // Extended thinking (Opus 4.5 feature)\n    if (this.config.extendedThinking?.enabled) {\n      body.thinking = {\n        type: \"enabled\",\n        budget_tokens: this.config.extendedThinking.budgetTokens,\n      };\n      // Extended thinking requires temperature 1\n      body.temperature = 1;\n    } else {\n      body.temperature = this.config.temperature;\n    }\n\n    // Build headers\n    const headers: Record<string, string> = {\n      \"Content-Type\": \"application/json\",\n      \"x-api-key\": this.config.apiKey,\n      \"anthropic-version\": \"2023-06-01\",\n    };\n\n    // Enable prompt caching beta\n    if (this.config.promptCaching) {\n      headers[\"anthropic-beta\"] = \"prompt-caching-2024-07-31\";\n    }\n\n    // Enable extended thinking beta\n    if (this.config.extendedThinking?.enabled) {\n      const existingBeta = headers[\"anthropic-beta\"];\n      headers[\"anthropic-beta\"] = existingBeta\n        ? `${existingBeta},interleaved-thinking-2025-05-14`\n        : \"interleaved-thinking-2025-05-14\";\n    }\n\n    const response = await fetch(\"https://api.anthropic.com/v1/messages\", {\n      method: \"POST\",\n      headers,\n      body: JSON.stringify(body),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Anthropic API error: ${response.status} - ${error}`);\n    }\n\n    const data = (await response.json()) as AnthropicResponse;\n\n    // Extract text and thinking from content blocks\n    let content = \"\";\n    let thinking = \"\";\n\n    for (const block of data.content) {\n      if (block.type === \"text\" && block.text) {\n        content += block.text;\n      } else if (block.type === \"thinking\" && block.thinking) {\n        thinking += block.thinking;\n      }\n    }\n\n    // Calculate tokens\n    const inputTokens = data.usage?.input_tokens ?? 0;\n    const outputTokens = data.usage?.output_tokens ?? 0;\n    const cachedTokens =\n      (data.usage?.cache_read_input_tokens ?? 0) +\n      (data.usage?.cache_creation_input_tokens ?? 0);\n\n    return {\n      content,\n      thinking: thinking || undefined,\n      usage: {\n        promptTokens: inputTokens,\n        completionTokens: outputTokens,\n        cachedTokens: cachedTokens || undefined,\n        totalTokens: inputTokens + outputTokens,\n      },\n      finishReason: data.stop_reason,\n      model: data.model,\n    };\n  }\n\n  // ===========================================================================\n  // GEMINI\n  // ===========================================================================\n\n  private async chatGemini(messages: LLMMessage[]): Promise<LLMResponse> {\n    const systemMessage = messages.find((m) => m.role === \"system\");\n    const contents = messages\n      .filter((m) => m.role !== \"system\")\n      .map((m) => ({\n        role: m.role === \"assistant\" ? \"model\" : \"user\",\n        parts: [{ text: m.content }],\n      }));\n\n    const maxTokens = this.config.effort\n      ? EFFORT_MAX_TOKENS[this.config.effort]\n      : this.config.maxTokens;\n\n    const url = `https://generativelanguage.googleapis.com/v1beta/models/${this.model}:generateContent?key=${this.config.apiKey}`;\n\n    const response = await fetch(url, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({\n        contents,\n        systemInstruction: systemMessage\n          ? { parts: [{ text: systemMessage.content }] }\n          : undefined,\n        generationConfig: {\n          temperature: this.config.temperature,\n          maxOutputTokens: maxTokens,\n        },\n      }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Gemini API error: ${response.status} - ${error}`);\n    }\n\n    const data = (await response.json()) as GeminiResponse;\n    return {\n      content: data.candidates?.[0]?.content?.parts?.[0]?.text ?? \"\",\n      usage: {\n        promptTokens: data.usageMetadata?.promptTokenCount ?? 0,\n        completionTokens: data.usageMetadata?.candidatesTokenCount ?? 0,\n        totalTokens: data.usageMetadata?.totalTokenCount ?? 0,\n      },\n      model: this.model,\n    };\n  }\n\n  // ===========================================================================\n  // DEEPSEEK\n  // ===========================================================================\n\n  private async chatDeepSeek(messages: LLMMessage[]): Promise<LLMResponse> {\n    const maxTokens = this.config.effort\n      ? EFFORT_MAX_TOKENS[this.config.effort]\n      : this.config.maxTokens;\n\n    const response = await fetch(\"https://api.deepseek.com/chat/completions\", {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        Authorization: `Bearer ${this.config.apiKey}`,\n      },\n      body: JSON.stringify({\n        model: this.model,\n        messages,\n        temperature: this.config.temperature,\n        max_tokens: maxTokens,\n      }),\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`DeepSeek API error: ${response.status} - ${error}`);\n    }\n\n    const data = (await response.json()) as DeepSeekResponse;\n    return {\n      content: data.choices[0].message.content,\n      usage: {\n        promptTokens: data.usage?.prompt_tokens ?? 0,\n        completionTokens: data.usage?.completion_tokens ?? 0,\n        totalTokens: data.usage?.total_tokens ?? 0,\n      },\n      finishReason: data.choices[0].finish_reason,\n      model: data.model,\n    };\n  }\n}\n\n// =============================================================================\n// FACTORY FUNCTION\n// =============================================================================\n\nexport function createLLMClient(config: LLMConfig): LLMClient {\n  return new UniversalLLMClient(config);\n}\n\n// =============================================================================\n// QUICK HELPERS\n// =============================================================================\n\n/**\n * Create a client optimized for fast, cheap responses\n */\nexport function createTurboClient(apiKey: string): LLMClient {\n  return new UniversalLLMClient({\n    provider: \"anthropic\",\n    model: \"claude-haiku-3-5-20241022\",\n    apiKey,\n    temperature: 0.3,\n    maxTokens: 2048,\n    effort: \"low\",\n  });\n}\n\n/**\n * Create a client optimized for complex reasoning with Opus 4.5\n */\nexport function createPowerClient(apiKey: string): LLMClient {\n  return new UniversalLLMClient({\n    provider: \"anthropic\",\n    model: \"claude-opus-4-5-20251101\",\n    apiKey,\n    temperature: 0.5,\n    maxTokens: 16384,\n    effort: \"high\",\n    extendedThinking: {\n      enabled: true,\n      budgetTokens: 16000,\n    },\n    promptCaching: true,\n  });\n}\n\n/**\n * Create a balanced client for everyday use\n */\nexport function createBalancedClient(apiKey: string): LLMClient {\n  return new UniversalLLMClient({\n    provider: \"anthropic\",\n    model: \"claude-sonnet-4-20250514\",\n    apiKey,\n    temperature: 0.7,\n    maxTokens: 8192,\n    effort: \"medium\",\n    promptCaching: true,\n  });\n}\n\n// =============================================================================\n// MULTI-PROVIDER ROTATION CLIENT\n// =============================================================================\n\nexport interface ProviderConfig {\n  provider: LLMProvider;\n  apiKey: string;\n  model?: string;\n  weight?: number; // 1-10, higher = more frequent selection\n  costPer1kTokens?: number; // For cost optimization\n}\n\nexport interface RotationStrategy {\n  type: \"round-robin\" | \"weighted\" | \"cost-optimized\" | \"latency-optimized\";\n  fallbackOrder?: LLMProvider[];\n}\n\ninterface ProviderStats {\n  successCount: number;\n  failCount: number;\n  totalLatencyMs: number;\n  lastUsed: number;\n  lastError?: string;\n}\n\n/**\n * Smart multi-provider LLM client with rotation and fallback\n * Supports: Claude (Anthropic), Gemini (Google), DeepSeek, OpenAI\n */\nexport class RotatingLLMClient implements LLMClient {\n  private providers: Map<LLMProvider, UniversalLLMClient> = new Map();\n  private providerConfigs: ProviderConfig[];\n  private strategy: RotationStrategy;\n  private stats: Map<LLMProvider, ProviderStats> = new Map();\n  private currentIndex = 0;\n  private baseConfig: Partial<LLMConfig>;\n\n  constructor(\n    providers: ProviderConfig[],\n    strategy: RotationStrategy = { type: \"round-robin\" },\n    baseConfig: Partial<LLMConfig> = {}\n  ) {\n    this.providerConfigs = providers;\n    this.strategy = strategy;\n    this.baseConfig = baseConfig;\n\n    // Initialize clients for each provider\n    for (const config of providers) {\n      const client = new UniversalLLMClient({\n        provider: config.provider,\n        apiKey: config.apiKey,\n        model: config.model,\n        temperature: baseConfig.temperature ?? 0.7,\n        maxTokens: baseConfig.maxTokens ?? 4096,\n        effort: baseConfig.effort,\n        extendedThinking: baseConfig.extendedThinking,\n        promptCaching: baseConfig.promptCaching,\n      });\n      this.providers.set(config.provider, client);\n      this.stats.set(config.provider, {\n        successCount: 0,\n        failCount: 0,\n        totalLatencyMs: 0,\n        lastUsed: 0,\n      });\n    }\n  }\n\n  getConfig(): LLMConfig {\n    const firstProvider = this.providerConfigs[0];\n    return {\n      provider: firstProvider.provider,\n      apiKey: firstProvider.apiKey,\n      model: firstProvider.model,\n      temperature: this.baseConfig.temperature ?? 0.7,\n      maxTokens: this.baseConfig.maxTokens ?? 4096,\n    };\n  }\n\n  async chat(messages: LLMMessage[]): Promise<LLMResponse> {\n    const orderedProviders = this.getProviderOrder();\n\n    for (const provider of orderedProviders) {\n      const client = this.providers.get(provider);\n      if (!client) continue;\n\n      try {\n        const startTime = Date.now();\n        const response = await client.chat(messages);\n        const latency = Date.now() - startTime;\n\n        // Update stats\n        const stats = this.stats.get(provider)!;\n        stats.successCount++;\n        stats.totalLatencyMs += latency;\n        stats.lastUsed = Date.now();\n\n        // Add provider info to response\n        response.model = `${provider}:${response.model}`;\n        return response;\n      } catch (error) {\n        // Update failure stats\n        const stats = this.stats.get(provider)!;\n        stats.failCount++;\n        stats.lastError = error instanceof Error ? error.message : String(error);\n\n        // Try next provider\n        continue;\n      }\n    }\n\n    throw new Error(\"All providers failed. Check API keys and quotas.\");\n  }\n\n  async complete(prompt: string): Promise<string> {\n    const response = await this.chat([{ role: \"user\", content: prompt }]);\n    return response.content;\n  }\n\n  private getProviderOrder(): LLMProvider[] {\n    const providers = this.providerConfigs.map((c) => c.provider);\n\n    switch (this.strategy.type) {\n      case \"round-robin\":\n        return this.roundRobinOrder(providers);\n      case \"weighted\":\n        return this.weightedOrder();\n      case \"cost-optimized\":\n        return this.costOptimizedOrder();\n      case \"latency-optimized\":\n        return this.latencyOptimizedOrder();\n      default:\n        return providers;\n    }\n  }\n\n  private roundRobinOrder(providers: LLMProvider[]): LLMProvider[] {\n    const result = [...providers];\n    // Rotate array based on current index\n    const rotated = [...result.slice(this.currentIndex), ...result.slice(0, this.currentIndex)];\n    this.currentIndex = (this.currentIndex + 1) % providers.length;\n    return rotated;\n  }\n\n  private weightedOrder(): LLMProvider[] {\n    // Sort by weight (higher first), with successful providers prioritized\n    return [...this.providerConfigs]\n      .sort((a, b) => {\n        const statsA = this.stats.get(a.provider)!;\n        const statsB = this.stats.get(b.provider)!;\n        const weightA = (a.weight ?? 5) * (statsA.failCount > 3 ? 0.5 : 1);\n        const weightB = (b.weight ?? 5) * (statsB.failCount > 3 ? 0.5 : 1);\n        return weightB - weightA;\n      })\n      .map((c) => c.provider);\n  }\n\n  private costOptimizedOrder(): LLMProvider[] {\n    // DeepSeek and Gemini are typically cheapest\n    const costOrder: Record<string, number> = {\n      deepseek: 1,\n      gemini: 2,\n      anthropic: 3,\n      openai: 4,\n    };\n    return [...this.providerConfigs]\n      .sort((a, b) => {\n        const costA = a.costPer1kTokens ?? costOrder[a.provider] ?? 5;\n        const costB = b.costPer1kTokens ?? costOrder[b.provider] ?? 5;\n        return costA - costB;\n      })\n      .map((c) => c.provider);\n  }\n\n  private latencyOptimizedOrder(): LLMProvider[] {\n    return [...this.providerConfigs]\n      .sort((a, b) => {\n        const statsA = this.stats.get(a.provider)!;\n        const statsB = this.stats.get(b.provider)!;\n        const avgA = statsA.successCount > 0 ? statsA.totalLatencyMs / statsA.successCount : 9999;\n        const avgB = statsB.successCount > 0 ? statsB.totalLatencyMs / statsB.successCount : 9999;\n        return avgA - avgB;\n      })\n      .map((c) => c.provider);\n  }\n\n  /**\n   * Get current provider statistics\n   */\n  getStats(): Record<string, ProviderStats> {\n    const result: Record<string, ProviderStats> = {};\n    for (const [provider, stats] of this.stats) {\n      result[provider] = { ...stats };\n    }\n    return result;\n  }\n\n  /**\n   * Reset all statistics\n   */\n  resetStats(): void {\n    for (const [provider] of this.stats) {\n      this.stats.set(provider, {\n        successCount: 0,\n        failCount: 0,\n        totalLatencyMs: 0,\n        lastUsed: 0,\n      });\n    }\n  }\n}\n\n// =============================================================================\n// ROTATION CLIENT FACTORY\n// =============================================================================\n\n/**\n * Create a rotating client from environment variables\n * Reads: ANTHROPIC_API_KEY, GEMINI_API_KEY, DEEPSEEK_API_KEY, OPENAI_API_KEY\n */\nexport function createRotatingClient(\n  strategy: RotationStrategy = { type: \"cost-optimized\" },\n  baseConfig: Partial<LLMConfig> = {}\n): RotatingLLMClient {\n  const providers: ProviderConfig[] = [];\n\n  // Check for each provider's API key in environment\n  const anthropicKey = process.env.ANTHROPIC_API_KEY;\n  const geminiKey = process.env.GEMINI_API_KEY;\n  const deepseekKey = process.env.DEEPSEEK_API_KEY;\n  const openaiKey = process.env.OPENAI_API_KEY;\n\n  if (anthropicKey) {\n    providers.push({\n      provider: \"anthropic\",\n      apiKey: anthropicKey,\n      weight: 8,\n      costPer1kTokens: 15, // Opus pricing\n    });\n  }\n\n  if (geminiKey) {\n    providers.push({\n      provider: \"gemini\",\n      apiKey: geminiKey,\n      weight: 7,\n      costPer1kTokens: 0, // Free tier\n    });\n  }\n\n  if (deepseekKey) {\n    providers.push({\n      provider: \"deepseek\",\n      apiKey: deepseekKey,\n      weight: 6,\n      costPer1kTokens: 0.14, // Very cheap\n    });\n  }\n\n  if (openaiKey) {\n    providers.push({\n      provider: \"openai\",\n      apiKey: openaiKey,\n      weight: 7,\n      costPer1kTokens: 5, // GPT-4o pricing\n    });\n  }\n\n  if (providers.length === 0) {\n    throw new Error(\n      \"No LLM API keys found. Set at least one of: ANTHROPIC_API_KEY, GEMINI_API_KEY, DEEPSEEK_API_KEY, OPENAI_API_KEY\"\n    );\n  }\n\n  return new RotatingLLMClient(providers, strategy, baseConfig);\n}\n\n/**\n * Create a brain client that smartly rotates between all available providers\n * Uses cost-optimized strategy by default (DeepSeek/Gemini first, Claude for complex tasks)\n */\nexport function createBrainClient(baseConfig: Partial<LLMConfig> = {}): RotatingLLMClient {\n  return createRotatingClient({ type: \"cost-optimized\" }, {\n    temperature: 0.7,\n    maxTokens: 8192,\n    ...baseConfig,\n  });\n}\n"],"mappings":";AAKA,SAAS,SAAS;AAMX,IAAM,oBAAoB,EAAE,KAAK,CAAC,UAAU,aAAa,UAAU,UAAU,CAAC;AAO9E,IAAM,oBAAoB,EAAE,KAAK,CAAC,OAAO,UAAU,MAAM,CAAC;AAO1D,IAAM,yBAAyB,EAAE,OAAO;AAAA,EAC7C,SAAS,EAAE,QAAQ,EAAE,QAAQ,KAAK;AAAA,EAClC,cAAc,EAAE,OAAO,EAAE,IAAI,GAAI,EAAE,IAAI,KAAM,EAAE,QAAQ,GAAI;AAC7D,CAAC;AAOM,IAAM,kBAAkB,EAAE,OAAO;AAAA,EACtC,UAAU;AAAA,EACV,OAAO,EAAE,OAAO,EAAE,SAAS;AAAA,EAC3B,QAAQ,EAAE,OAAO;AAAA,EACjB,aAAa,EAAE,OAAO,EAAE,IAAI,CAAC,EAAE,IAAI,CAAC,EAAE,QAAQ,GAAG;AAAA,EACjD,WAAW,EAAE,OAAO,EAAE,QAAQ,IAAI;AAAA;AAAA,EAGlC,QAAQ,kBAAkB,SAAS;AAAA,EACnC,kBAAkB,uBAAuB,SAAS;AAAA;AAAA,EAGlD,eAAe,EAAE,QAAQ,EAAE,SAAS;AACtC,CAAC;;;ACED,IAAM,iBAAyC;AAAA,EAC7C,QAAQ;AAAA,EACR,WAAW;AAAA,EACX,QAAQ;AAAA,EACR,UAAU;AACZ;AAGA,IAAM,oBAAiD;AAAA,EACrD,KAAK;AAAA,EACL,QAAQ;AAAA,EACR,MAAM;AACR;AAMO,IAAM,qBAAN,MAA8C;AAAA,EAC3C;AAAA,EACA;AAAA,EAER,YAAY,QAAmB;AAC7B,SAAK,SAAS;AACd,SAAK,QAAQ,OAAO,SAAS,eAAe,OAAO,QAAQ;AAAA,EAC7D;AAAA,EAEA,YAAuB;AACrB,WAAO,EAAE,GAAG,KAAK,OAAO;AAAA,EAC1B;AAAA,EAEA,MAAM,KAAK,UAA8C;AACvD,UAAM,YAAY,KAAK,IAAI;AAE3B,QAAI;AACJ,YAAQ,KAAK,OAAO,UAAU;AAAA,MAC5B,KAAK;AACH,mBAAW,MAAM,KAAK,WAAW,QAAQ;AACzC;AAAA,MACF,KAAK;AACH,mBAAW,MAAM,KAAK,cAAc,QAAQ;AAC5C;AAAA,MACF,KAAK;AACH,mBAAW,MAAM,KAAK,WAAW,QAAQ;AACzC;AAAA,MACF,KAAK;AACH,mBAAW,MAAM,KAAK,aAAa,QAAQ;AAC3C;AAAA,MACF;AACE,cAAM,IAAI,MAAM,yBAAyB,KAAK,OAAO,QAAQ,EAAE;AAAA,IACnE;AAEA,aAAS,YAAY,KAAK,IAAI,IAAI;AAClC,WAAO;AAAA,EACT;AAAA,EAEA,MAAM,SAAS,QAAiC;AAC9C,UAAM,WAAW,MAAM,KAAK,KAAK,CAAC,EAAE,MAAM,QAAQ,SAAS,OAAO,CAAC,CAAC;AACpE,WAAO,SAAS;AAAA,EAClB;AAAA;AAAA;AAAA;AAAA,EAMA,MAAc,WAAW,UAA8C;AACrE,UAAM,YAAY,KAAK,OAAO,SAC1B,kBAAkB,KAAK,OAAO,MAAM,IACpC,KAAK,OAAO;AAEhB,UAAM,WAAW,MAAM,MAAM,8CAA8C;AAAA,MACzE,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,gBAAgB;AAAA,QAChB,eAAe,UAAU,KAAK,OAAO,MAAM;AAAA,MAC7C;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACnB,OAAO,KAAK;AAAA,QACZ;AAAA,QACA,aAAa,KAAK,OAAO;AAAA,QACzB,YAAY;AAAA,MACd,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,QAAQ,MAAM,SAAS,KAAK;AAClC,YAAM,IAAI,MAAM,qBAAqB,SAAS,MAAM,MAAM,KAAK,EAAE;AAAA,IACnE;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,WAAO;AAAA,MACL,SAAS,KAAK,QAAQ,CAAC,EAAE,QAAQ;AAAA,MACjC,OAAO;AAAA,QACL,cAAc,KAAK,OAAO,iBAAiB;AAAA,QAC3C,kBAAkB,KAAK,OAAO,qBAAqB;AAAA,QACnD,aAAa,KAAK,OAAO,gBAAgB;AAAA,MAC3C;AAAA,MACA,cAAc,KAAK,QAAQ,CAAC,EAAE;AAAA,MAC9B,OAAO,KAAK;AAAA,IACd;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAMA,MAAc,cAAc,UAA8C;AACxE,UAAM,gBAAgB,SAAS,KAAK,CAAC,MAAM,EAAE,SAAS,QAAQ;AAC9D,UAAM,eAAe,SAAS,OAAO,CAAC,MAAM,EAAE,SAAS,QAAQ;AAG/D,UAAM,OAAgC;AAAA,MACpC,OAAO,KAAK;AAAA,MACZ,YAAY,KAAK,OAAO;AAAA,MACxB,UAAU,aAAa,IAAI,CAAC,OAAO;AAAA,QACjC,MAAM,EAAE,SAAS,cAAc,cAAc;AAAA,QAC7C,SAAS,EAAE;AAAA,MACb,EAAE;AAAA,IACJ;AAGA,QAAI,eAAe;AACjB,UAAI,KAAK,OAAO,eAAe;AAC7B,aAAK,SAAS;AAAA,UACZ;AAAA,YACE,MAAM;AAAA,YACN,MAAM,cAAc;AAAA,YACpB,eAAe,EAAE,MAAM,YAAY;AAAA,UACrC;AAAA,QACF;AAAA,MACF,OAAO;AACL,aAAK,SAAS,cAAc;AAAA,MAC9B;AAAA,IACF;AAGA,QAAI,KAAK,OAAO,kBAAkB,SAAS;AACzC,WAAK,WAAW;AAAA,QACd,MAAM;AAAA,QACN,eAAe,KAAK,OAAO,iBAAiB;AAAA,MAC9C;AAEA,WAAK,cAAc;AAAA,IACrB,OAAO;AACL,WAAK,cAAc,KAAK,OAAO;AAAA,IACjC;AAGA,UAAM,UAAkC;AAAA,MACtC,gBAAgB;AAAA,MAChB,aAAa,KAAK,OAAO;AAAA,MACzB,qBAAqB;AAAA,IACvB;AAGA,QAAI,KAAK,OAAO,eAAe;AAC7B,cAAQ,gBAAgB,IAAI;AAAA,IAC9B;AAGA,QAAI,KAAK,OAAO,kBAAkB,SAAS;AACzC,YAAM,eAAe,QAAQ,gBAAgB;AAC7C,cAAQ,gBAAgB,IAAI,eACxB,GAAG,YAAY,qCACf;AAAA,IACN;AAEA,UAAM,WAAW,MAAM,MAAM,yCAAyC;AAAA,MACpE,QAAQ;AAAA,MACR;AAAA,MACA,MAAM,KAAK,UAAU,IAAI;AAAA,IAC3B,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,QAAQ,MAAM,SAAS,KAAK;AAClC,YAAM,IAAI,MAAM,wBAAwB,SAAS,MAAM,MAAM,KAAK,EAAE;AAAA,IACtE;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAGlC,QAAI,UAAU;AACd,QAAI,WAAW;AAEf,eAAW,SAAS,KAAK,SAAS;AAChC,UAAI,MAAM,SAAS,UAAU,MAAM,MAAM;AACvC,mBAAW,MAAM;AAAA,MACnB,WAAW,MAAM,SAAS,cAAc,MAAM,UAAU;AACtD,oBAAY,MAAM;AAAA,MACpB;AAAA,IACF;AAGA,UAAM,cAAc,KAAK,OAAO,gBAAgB;AAChD,UAAM,eAAe,KAAK,OAAO,iBAAiB;AAClD,UAAM,gBACH,KAAK,OAAO,2BAA2B,MACvC,KAAK,OAAO,+BAA+B;AAE9C,WAAO;AAAA,MACL;AAAA,MACA,UAAU,YAAY;AAAA,MACtB,OAAO;AAAA,QACL,cAAc;AAAA,QACd,kBAAkB;AAAA,QAClB,cAAc,gBAAgB;AAAA,QAC9B,aAAa,cAAc;AAAA,MAC7B;AAAA,MACA,cAAc,KAAK;AAAA,MACnB,OAAO,KAAK;AAAA,IACd;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAMA,MAAc,WAAW,UAA8C;AACrE,UAAM,gBAAgB,SAAS,KAAK,CAAC,MAAM,EAAE,SAAS,QAAQ;AAC9D,UAAM,WAAW,SACd,OAAO,CAAC,MAAM,EAAE,SAAS,QAAQ,EACjC,IAAI,CAAC,OAAO;AAAA,MACX,MAAM,EAAE,SAAS,cAAc,UAAU;AAAA,MACzC,OAAO,CAAC,EAAE,MAAM,EAAE,QAAQ,CAAC;AAAA,IAC7B,EAAE;AAEJ,UAAM,YAAY,KAAK,OAAO,SAC1B,kBAAkB,KAAK,OAAO,MAAM,IACpC,KAAK,OAAO;AAEhB,UAAM,MAAM,2DAA2D,KAAK,KAAK,wBAAwB,KAAK,OAAO,MAAM;AAE3H,UAAM,WAAW,MAAM,MAAM,KAAK;AAAA,MAChC,QAAQ;AAAA,MACR,SAAS,EAAE,gBAAgB,mBAAmB;AAAA,MAC9C,MAAM,KAAK,UAAU;AAAA,QACnB;AAAA,QACA,mBAAmB,gBACf,EAAE,OAAO,CAAC,EAAE,MAAM,cAAc,QAAQ,CAAC,EAAE,IAC3C;AAAA,QACJ,kBAAkB;AAAA,UAChB,aAAa,KAAK,OAAO;AAAA,UACzB,iBAAiB;AAAA,QACnB;AAAA,MACF,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,QAAQ,MAAM,SAAS,KAAK;AAClC,YAAM,IAAI,MAAM,qBAAqB,SAAS,MAAM,MAAM,KAAK,EAAE;AAAA,IACnE;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,WAAO;AAAA,MACL,SAAS,KAAK,aAAa,CAAC,GAAG,SAAS,QAAQ,CAAC,GAAG,QAAQ;AAAA,MAC5D,OAAO;AAAA,QACL,cAAc,KAAK,eAAe,oBAAoB;AAAA,QACtD,kBAAkB,KAAK,eAAe,wBAAwB;AAAA,QAC9D,aAAa,KAAK,eAAe,mBAAmB;AAAA,MACtD;AAAA,MACA,OAAO,KAAK;AAAA,IACd;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAMA,MAAc,aAAa,UAA8C;AACvE,UAAM,YAAY,KAAK,OAAO,SAC1B,kBAAkB,KAAK,OAAO,MAAM,IACpC,KAAK,OAAO;AAEhB,UAAM,WAAW,MAAM,MAAM,6CAA6C;AAAA,MACxE,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,gBAAgB;AAAA,QAChB,eAAe,UAAU,KAAK,OAAO,MAAM;AAAA,MAC7C;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACnB,OAAO,KAAK;AAAA,QACZ;AAAA,QACA,aAAa,KAAK,OAAO;AAAA,QACzB,YAAY;AAAA,MACd,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,QAAQ,MAAM,SAAS,KAAK;AAClC,YAAM,IAAI,MAAM,uBAAuB,SAAS,MAAM,MAAM,KAAK,EAAE;AAAA,IACrE;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,WAAO;AAAA,MACL,SAAS,KAAK,QAAQ,CAAC,EAAE,QAAQ;AAAA,MACjC,OAAO;AAAA,QACL,cAAc,KAAK,OAAO,iBAAiB;AAAA,QAC3C,kBAAkB,KAAK,OAAO,qBAAqB;AAAA,QACnD,aAAa,KAAK,OAAO,gBAAgB;AAAA,MAC3C;AAAA,MACA,cAAc,KAAK,QAAQ,CAAC,EAAE;AAAA,MAC9B,OAAO,KAAK;AAAA,IACd;AAAA,EACF;AACF;AAMO,SAAS,gBAAgB,QAA8B;AAC5D,SAAO,IAAI,mBAAmB,MAAM;AACtC;AASO,SAAS,kBAAkB,QAA2B;AAC3D,SAAO,IAAI,mBAAmB;AAAA,IAC5B,UAAU;AAAA,IACV,OAAO;AAAA,IACP;AAAA,IACA,aAAa;AAAA,IACb,WAAW;AAAA,IACX,QAAQ;AAAA,EACV,CAAC;AACH;AAKO,SAAS,kBAAkB,QAA2B;AAC3D,SAAO,IAAI,mBAAmB;AAAA,IAC5B,UAAU;AAAA,IACV,OAAO;AAAA,IACP;AAAA,IACA,aAAa;AAAA,IACb,WAAW;AAAA,IACX,QAAQ;AAAA,IACR,kBAAkB;AAAA,MAChB,SAAS;AAAA,MACT,cAAc;AAAA,IAChB;AAAA,IACA,eAAe;AAAA,EACjB,CAAC;AACH;AAKO,SAAS,qBAAqB,QAA2B;AAC9D,SAAO,IAAI,mBAAmB;AAAA,IAC5B,UAAU;AAAA,IACV,OAAO;AAAA,IACP;AAAA,IACA,aAAa;AAAA,IACb,WAAW;AAAA,IACX,QAAQ;AAAA,IACR,eAAe;AAAA,EACjB,CAAC;AACH;AA+BO,IAAM,oBAAN,MAA6C;AAAA,EAC1C,YAAkD,oBAAI,IAAI;AAAA,EAC1D;AAAA,EACA;AAAA,EACA,QAAyC,oBAAI,IAAI;AAAA,EACjD,eAAe;AAAA,EACf;AAAA,EAER,YACE,WACA,WAA6B,EAAE,MAAM,cAAc,GACnD,aAAiC,CAAC,GAClC;AACA,SAAK,kBAAkB;AACvB,SAAK,WAAW;AAChB,SAAK,aAAa;AAGlB,eAAW,UAAU,WAAW;AAC9B,YAAM,SAAS,IAAI,mBAAmB;AAAA,QACpC,UAAU,OAAO;AAAA,QACjB,QAAQ,OAAO;AAAA,QACf,OAAO,OAAO;AAAA,QACd,aAAa,WAAW,eAAe;AAAA,QACvC,WAAW,WAAW,aAAa;AAAA,QACnC,QAAQ,WAAW;AAAA,QACnB,kBAAkB,WAAW;AAAA,QAC7B,eAAe,WAAW;AAAA,MAC5B,CAAC;AACD,WAAK,UAAU,IAAI,OAAO,UAAU,MAAM;AAC1C,WAAK,MAAM,IAAI,OAAO,UAAU;AAAA,QAC9B,cAAc;AAAA,QACd,WAAW;AAAA,QACX,gBAAgB;AAAA,QAChB,UAAU;AAAA,MACZ,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,YAAuB;AACrB,UAAM,gBAAgB,KAAK,gBAAgB,CAAC;AAC5C,WAAO;AAAA,MACL,UAAU,cAAc;AAAA,MACxB,QAAQ,cAAc;AAAA,MACtB,OAAO,cAAc;AAAA,MACrB,aAAa,KAAK,WAAW,eAAe;AAAA,MAC5C,WAAW,KAAK,WAAW,aAAa;AAAA,IAC1C;AAAA,EACF;AAAA,EAEA,MAAM,KAAK,UAA8C;AACvD,UAAM,mBAAmB,KAAK,iBAAiB;AAE/C,eAAW,YAAY,kBAAkB;AACvC,YAAM,SAAS,KAAK,UAAU,IAAI,QAAQ;AAC1C,UAAI,CAAC,OAAQ;AAEb,UAAI;AACF,cAAM,YAAY,KAAK,IAAI;AAC3B,cAAM,WAAW,MAAM,OAAO,KAAK,QAAQ;AAC3C,cAAM,UAAU,KAAK,IAAI,IAAI;AAG7B,cAAM,QAAQ,KAAK,MAAM,IAAI,QAAQ;AACrC,cAAM;AACN,cAAM,kBAAkB;AACxB,cAAM,WAAW,KAAK,IAAI;AAG1B,iBAAS,QAAQ,GAAG,QAAQ,IAAI,SAAS,KAAK;AAC9C,eAAO;AAAA,MACT,SAAS,OAAO;AAEd,cAAM,QAAQ,KAAK,MAAM,IAAI,QAAQ;AACrC,cAAM;AACN,cAAM,YAAY,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAGvE;AAAA,MACF;AAAA,IACF;AAEA,UAAM,IAAI,MAAM,kDAAkD;AAAA,EACpE;AAAA,EAEA,MAAM,SAAS,QAAiC;AAC9C,UAAM,WAAW,MAAM,KAAK,KAAK,CAAC,EAAE,MAAM,QAAQ,SAAS,OAAO,CAAC,CAAC;AACpE,WAAO,SAAS;AAAA,EAClB;AAAA,EAEQ,mBAAkC;AACxC,UAAM,YAAY,KAAK,gBAAgB,IAAI,CAAC,MAAM,EAAE,QAAQ;AAE5D,YAAQ,KAAK,SAAS,MAAM;AAAA,MAC1B,KAAK;AACH,eAAO,KAAK,gBAAgB,SAAS;AAAA,MACvC,KAAK;AACH,eAAO,KAAK,cAAc;AAAA,MAC5B,KAAK;AACH,eAAO,KAAK,mBAAmB;AAAA,MACjC,KAAK;AACH,eAAO,KAAK,sBAAsB;AAAA,MACpC;AACE,eAAO;AAAA,IACX;AAAA,EACF;AAAA,EAEQ,gBAAgB,WAAyC;AAC/D,UAAM,SAAS,CAAC,GAAG,SAAS;AAE5B,UAAM,UAAU,CAAC,GAAG,OAAO,MAAM,KAAK,YAAY,GAAG,GAAG,OAAO,MAAM,GAAG,KAAK,YAAY,CAAC;AAC1F,SAAK,gBAAgB,KAAK,eAAe,KAAK,UAAU;AACxD,WAAO;AAAA,EACT;AAAA,EAEQ,gBAA+B;AAErC,WAAO,CAAC,GAAG,KAAK,eAAe,EAC5B,KAAK,CAAC,GAAG,MAAM;AACd,YAAM,SAAS,KAAK,MAAM,IAAI,EAAE,QAAQ;AACxC,YAAM,SAAS,KAAK,MAAM,IAAI,EAAE,QAAQ;AACxC,YAAM,WAAW,EAAE,UAAU,MAAM,OAAO,YAAY,IAAI,MAAM;AAChE,YAAM,WAAW,EAAE,UAAU,MAAM,OAAO,YAAY,IAAI,MAAM;AAChE,aAAO,UAAU;AAAA,IACnB,CAAC,EACA,IAAI,CAAC,MAAM,EAAE,QAAQ;AAAA,EAC1B;AAAA,EAEQ,qBAAoC;AAE1C,UAAM,YAAoC;AAAA,MACxC,UAAU;AAAA,MACV,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,QAAQ;AAAA,IACV;AACA,WAAO,CAAC,GAAG,KAAK,eAAe,EAC5B,KAAK,CAAC,GAAG,MAAM;AACd,YAAM,QAAQ,EAAE,mBAAmB,UAAU,EAAE,QAAQ,KAAK;AAC5D,YAAM,QAAQ,EAAE,mBAAmB,UAAU,EAAE,QAAQ,KAAK;AAC5D,aAAO,QAAQ;AAAA,IACjB,CAAC,EACA,IAAI,CAAC,MAAM,EAAE,QAAQ;AAAA,EAC1B;AAAA,EAEQ,wBAAuC;AAC7C,WAAO,CAAC,GAAG,KAAK,eAAe,EAC5B,KAAK,CAAC,GAAG,MAAM;AACd,YAAM,SAAS,KAAK,MAAM,IAAI,EAAE,QAAQ;AACxC,YAAM,SAAS,KAAK,MAAM,IAAI,EAAE,QAAQ;AACxC,YAAM,OAAO,OAAO,eAAe,IAAI,OAAO,iBAAiB,OAAO,eAAe;AACrF,YAAM,OAAO,OAAO,eAAe,IAAI,OAAO,iBAAiB,OAAO,eAAe;AACrF,aAAO,OAAO;AAAA,IAChB,CAAC,EACA,IAAI,CAAC,MAAM,EAAE,QAAQ;AAAA,EAC1B;AAAA;AAAA;AAAA;AAAA,EAKA,WAA0C;AACxC,UAAM,SAAwC,CAAC;AAC/C,eAAW,CAAC,UAAU,KAAK,KAAK,KAAK,OAAO;AAC1C,aAAO,QAAQ,IAAI,EAAE,GAAG,MAAM;AAAA,IAChC;AACA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,aAAmB;AACjB,eAAW,CAAC,QAAQ,KAAK,KAAK,OAAO;AACnC,WAAK,MAAM,IAAI,UAAU;AAAA,QACvB,cAAc;AAAA,QACd,WAAW;AAAA,QACX,gBAAgB;AAAA,QAChB,UAAU;AAAA,MACZ,CAAC;AAAA,IACH;AAAA,EACF;AACF;AAUO,SAAS,qBACd,WAA6B,EAAE,MAAM,iBAAiB,GACtD,aAAiC,CAAC,GACf;AACnB,QAAM,YAA8B,CAAC;AAGrC,QAAM,eAAe,QAAQ,IAAI;AACjC,QAAM,YAAY,QAAQ,IAAI;AAC9B,QAAM,cAAc,QAAQ,IAAI;AAChC,QAAM,YAAY,QAAQ,IAAI;AAE9B,MAAI,cAAc;AAChB,cAAU,KAAK;AAAA,MACb,UAAU;AAAA,MACV,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,iBAAiB;AAAA;AAAA,IACnB,CAAC;AAAA,EACH;AAEA,MAAI,WAAW;AACb,cAAU,KAAK;AAAA,MACb,UAAU;AAAA,MACV,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,iBAAiB;AAAA;AAAA,IACnB,CAAC;AAAA,EACH;AAEA,MAAI,aAAa;AACf,cAAU,KAAK;AAAA,MACb,UAAU;AAAA,MACV,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,iBAAiB;AAAA;AAAA,IACnB,CAAC;AAAA,EACH;AAEA,MAAI,WAAW;AACb,cAAU,KAAK;AAAA,MACb,UAAU;AAAA,MACV,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,iBAAiB;AAAA;AAAA,IACnB,CAAC;AAAA,EACH;AAEA,MAAI,UAAU,WAAW,GAAG;AAC1B,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAEA,SAAO,IAAI,kBAAkB,WAAW,UAAU,UAAU;AAC9D;AAMO,SAAS,kBAAkB,aAAiC,CAAC,GAAsB;AACxF,SAAO,qBAAqB,EAAE,MAAM,iBAAiB,GAAG;AAAA,IACtD,aAAa;AAAA,IACb,WAAW;AAAA,IACX,GAAG;AAAA,EACL,CAAC;AACH;","names":[]}