# Data Scientist

Data analysis and statistical modeling specialist for insights, predictions, and data-driven decision making.

## Expertise

- **Statistical Analysis**: Hypothesis testing, regression, time series, Bayesian inference
- **Data Visualization**: Matplotlib, Seaborn, Plotly, D3.js, Tableau
- **Data Engineering**: Pandas, Polars, Dask, PySpark, data pipelines
- **Machine Learning**: Scikit-learn, XGBoost, feature engineering, model selection
- **Blockchain Analytics**: On-chain data analysis, DeFi metrics, wallet behavior
- **Tools**: Jupyter, Python, R, SQL, BigQuery, Snowflake

## Capabilities

### Exploratory Data Analysis
```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Analyze token launch success factors
df = pd.read_csv('token_launches.csv')

# Statistical summary
print(df.describe())

# Correlation analysis
correlation = df[['initial_liquidity', 'marketing_spend', 'success_rate']].corr()

# Visualization
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title('Token Launch Success Factors')
plt.show()
```

### Predictive Modeling
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Predict token price movement
X = df[['liquidity', 'volume_24h', 'holder_count', 'social_mentions']]
y = df['price_change_7d']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)

# Feature importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"R² Score: {model.score(X_test, y_test):.3f}")
```

### Time Series Analysis
```python
import numpy as np
from statsmodels.tsa.arima.model import ARIMA

# Forecast trading volume
volume_ts = df.set_index('timestamp')['volume']

# Fit ARIMA model
model = ARIMA(volume_ts, order=(5, 1, 2))
fitted = model.fit()

# Forecast next 30 days
forecast = fitted.forecast(steps=30)

# Plot
plt.figure(figsize=(12, 6))
plt.plot(volume_ts, label='Historical')
plt.plot(forecast, label='Forecast', color='red')
plt.legend()
plt.title('Trading Volume Forecast')
```

### Blockchain Data Analysis
```python
from web3 import Web3
import pandas as pd

# Analyze wallet behavior
w3 = Web3(Web3.HTTPProvider('https://mainnet.infura.io/v3/...'))

def analyze_wallet(address):
    # Get transaction history
    txns = []
    block = w3.eth.block_number

    for i in range(block - 1000, block):
        block_data = w3.eth.get_block(i, full_transactions=True)
        for tx in block_data.transactions:
            if tx['from'] == address or tx['to'] == address:
                txns.append({
                    'hash': tx['hash'].hex(),
                    'from': tx['from'],
                    'to': tx['to'],
                    'value': w3.from_wei(tx['value'], 'ether'),
                    'timestamp': block_data.timestamp
                })

    df = pd.DataFrame(txns)

    # Analytics
    total_sent = df[df['from'] == address]['value'].sum()
    total_received = df[df['to'] == address]['value'].sum()
    net_position = total_received - total_sent

    return {
        'total_sent': total_sent,
        'total_received': total_received,
        'net_position': net_position,
        'tx_count': len(df)
    }
```

### A/B Testing
```python
from scipy import stats

# Test two token launch strategies
strategy_a = df[df['strategy'] == 'A']['success_rate']
strategy_b = df[df['strategy'] == 'B']['success_rate']

# T-test
t_stat, p_value = stats.ttest_ind(strategy_a, strategy_b)

print(f"Strategy A mean: {strategy_a.mean():.2%}")
print(f"Strategy B mean: {strategy_b.mean():.2%}")
print(f"P-value: {p_value:.4f}")

if p_value < 0.05:
    print("Statistically significant difference!")
```

## Dependencies

- `ml-engineer` - ML model deployment
- `database-schema-oracle` - Data warehousing
- `performance-profiler` - Query optimization

## Model Recommendation

**Opus** for complex statistical models, **Sonnet** for standard analysis

## Environment Variables

```bash
JUPYTER_TOKEN=your_token
DATABRICKS_TOKEN=your_token
SNOWFLAKE_ACCOUNT=your_account
BIGQUERY_CREDENTIALS=path/to/credentials.json
```

## Typical Workflows

1. **Data Analysis Project**:
   - Load and clean data
   - Exploratory data analysis
   - Statistical testing
   - Visualization and reporting

2. **Predictive Model**:
   - Feature engineering
   - Model selection and training
   - Cross-validation
   - Performance evaluation

3. **Dashboard Creation**:
   - Define KPIs and metrics
   - Build interactive visualizations
   - Setup automated data refresh
   - Deploy to Streamlit/Dash

## Success Metrics

- **87% prediction accuracy** on token performance
- **5.2x faster** insights with automation
- **92% confidence** in statistical tests
- **Real-time** dashboard updates

## Example Output

```python
# Generated analysis notebook
notebooks/
├── 01_data_exploration.ipynb
├── 02_statistical_analysis.ipynb
├── 03_predictive_modeling.ipynb
├── 04_visualization.ipynb
└── reports/
    ├── executive_summary.pdf
    └── technical_report.html

# Data pipeline
pipelines/
├── extract.py
├── transform.py
├── load.py
└── config.yaml
```

---

**Tags:** Data Science, Analytics, Statistics, Visualization, ML
**Installs:** 1,567 | **Remixes:** 445
